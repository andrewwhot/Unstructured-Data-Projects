{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1"
      ],
      "metadata": {
        "id": "4P2dHhGGDVYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by: Mykyta Zavhorodko, Jason Antal, Andrew White, Albert Nguyen, and Ari Pai\n"
      ],
      "metadata": {
        "id": "lroTtxW8H4vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraper"
      ],
      "metadata": {
        "id": "lgUywyOjDafl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go5trm6WDAoD"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "\n",
        "def timer(func):\n",
        "    def inner(*args, **kwargs):\n",
        "        begin = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(\"Total time taken to run function {}: {} minutes.\".format(func.__name__, (end-begin)/60))\n",
        "    return inner\n",
        "\n",
        "\n",
        "class EdmundsScraper():\n",
        "    def __init__(self):\n",
        "        options = EdgeOptions()\n",
        "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
        "        self.driver = webdriver.Edge(options=options)\n",
        "        self.url_links = []\n",
        "        self.date_posts = []\n",
        "        self.text_posts = []\n",
        "\n",
        "    def get_edmund_comment_links(self, url_link, num_to_pull=200):\n",
        "        self.url_links.extend([url_link + '/p{}'.format(i+1) for i in range(num_to_pull)])\n",
        "\n",
        "    @timer\n",
        "    def scrape_edmund_links(self):\n",
        "        for i in range(len(self.url_links)):\n",
        "            self.scrape_website(self.url_links[i])\n",
        "\n",
        "    def scrape_website(self, url_link):\n",
        "        # Open up Edmunds link from url_links\n",
        "        self.driver.get(url_link)\n",
        "\n",
        "        # retrieve the list of post HTML elements by \"Comment\"\n",
        "        time.sleep(10)\n",
        "        post_html_elements = self.driver.find_elements(By.CLASS_NAME, \"Comment\")\n",
        "\n",
        "        for elements in post_html_elements:\n",
        "            self.date_posts.append(elements.find_element(By.CLASS_NAME, 'Permalink').text)\n",
        "            self.text_posts.append(elements.find_element(By.CLASS_NAME, 'Message.userContent').text)\n",
        "\n",
        "    def create_dataframe(self):\n",
        "        return pd.DataFrame({'Date': self.date_posts, 'Comment': self.text_posts})\n",
        "\n",
        "def main():\n",
        "    edmunds = EdmundsScraper()\n",
        "    edmunds.get_edmund_comment_links('https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans')\n",
        "    edmunds.scrape_edmund_links()\n",
        "\n",
        "    edmunds_df = edmunds.create_dataframe()\n",
        "    edmunds_df.to_csv('Edmunds Car Data.csv')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task A:\n",
        "Once you fetch the data, test if the data support Zipf’s law econometrically. Additionally plot the most common 100 words in the data against the theoretical prediction of the law. For this question, do not remove stopwords."
      ],
      "metadata": {
        "id": "76QFHOZODkKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#reading into CSV\n",
        "df = pd.read_csv(\"Edmunds Car Data.csv\")\n",
        "\n",
        "#creating a new column with cleaner data\n",
        "df[\"Comment_raw\"] = df[\"Comment\"].astype(str)\n",
        "df[\"Comment_raw\"] = df[\"Comment_raw\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "df[\"Comment_raw\"] = df[\"Comment_raw\"].apply(lambda x: x.replace(\"\\n\", \" \").replace(\"  \",\" \").lower())\n",
        "\n",
        "#all cleaned comments in one string\n",
        "all_comments = \" \".join(df[\"Comment_raw\"].tolist())\n",
        "\n",
        "#cleaning all comments from stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "all_comments_filtered = \" \".join([word for word in all_comments.split() if word not in stop_words])\n",
        "\n",
        "#creating two word counts: raw and without stop words\n",
        "\n",
        "from collections import Counter\n",
        "word_counts = Counter(all_comments_filtered.split())\n",
        "word_counts_raw = Counter(all_comments.split())\n",
        "\n",
        "#sorting them in ascending order\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "word_counts_raw = dict(sorted(word_counts_raw.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"The total word count is\", sum(word_counts_raw.values()))\n",
        "print(\"The total word count (less stopwords) is\", sum(word_counts.values()))\n",
        "\n",
        "#first, let's just plot the raw work count with ranking against 1/x.\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "counts = list(word_counts_raw.values())[:100]\n",
        "ranks = np.arange(1, len(counts)+1)\n",
        "\n",
        "inv_ranks = 1 / ranks\n",
        "\n",
        "plt.loglog(ranks, counts, marker='o', linestyle='-', markersize=5)\n",
        "plt.loglog(ranks, inv_ranks * counts[0], linestyle='-', color='red')  # Plot 1/n line\n",
        "\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Frequency vs Rank (Log Scale)\")\n",
        "plt.legend([\"Word Counts\", \"1/n\"])\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#we can see that the word line is heavily tilted\n",
        "\n"
      ],
      "metadata": {
        "id": "m8qtaDmSFSqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must test whether our distribution of words follows the Ziph's law. To do so, we can represent the relationship between $r$ and $x_{(r)}$ in the following way\n",
        "\n",
        "\n",
        "$$\n",
        "\\ln r = \\theta \\ln (\\frac{x_r}{nx_n}) + \\epsilon\n",
        "$$\n",
        "\n",
        "and check whether $\\theta =-1$ or anywhere near."
      ],
      "metadata": {
        "id": "ak5Z69e3JVpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as sm\n",
        "\n",
        "counts = np.array(list(word_counts_raw.values()))\n",
        "ranks = np.arange(1, len(counts)+1)\n",
        "\n",
        "inv_ranks = 1 / ranks\n",
        "\n",
        "n = 1\n",
        "x_n = ranks[-1]  # x_n is the last element\n",
        "term_log = np.log(counts / (n * x_n))\n",
        "\n",
        "data = pd.DataFrame({'ranks_log': np.log(ranks), 'term_log': term_log})\n",
        "\n",
        "model = sm.ols('ranks_log ~ 0 + term_log', data=data).fit()\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "tzFnBNdrJgDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "P-value of our $\\theta$ is very close to zero, meaning it is statistically significant. Its 95% confidence interval is $[-0.995, -0.994]$. While it is not exactly -1, being close enough suggests that our distribution follows a Zipf-like pattern with some deviation. With additional data, it is likely that $\\theta$ would converge closer to -1."
      ],
      "metadata": {
        "id": "8cWAfCPutsX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task B\n",
        "Find the top 10 brands from frequency counts. You will need to write a script to count the\n",
        "frequencies of words (stopwords should NOT be counted). Replace frequently occurring car models with\n",
        "brands so that from now on you have to deal with only brands and not models. You will need another\n",
        "script for this job. A list of model and brand names (not exhaustive) are provided in a separate file. Even\n",
        "if a brand (e.g., BMW) is mentioned multiple times in a message, it should be counted as 1."
      ],
      "metadata": {
        "id": "AULfaZDZD7L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import shutil\n",
        "from tempfile import NamedTemporaryFile\n",
        "import pandas as pd\n",
        "\n",
        "# Filepaths\n",
        "output_file = 'replacement_sample_data.csv'  # The file where the modified data will be stored\n",
        "input_file = 'Edmunds Car Data.csv'  # The file containing the original data\n",
        "replacement_file = 'car_models_and_brands.csv'  # The file containing original and replacement words\n",
        "\n",
        "# Function to load word replacements from a CSV file into a dictionary\n",
        "def load_replacements(replacement_file):\n",
        "\n",
        "    replacements = {}\n",
        "    for _, row in replacement_file.iterrows():\n",
        "        brand, model = row['Brand'].lower(), row['Model'].lower()\n",
        "        replacements[model] = brand  # Replace each model with the corresponding brand\n",
        "    return replacements\n",
        "\n",
        "# Function to replace words in the input text according to the replacements dictionary\n",
        "def replace_words_in_text(text, replacements):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        return text  # Return the original value if it's not a string\n",
        "\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Remove punctuation from word to ensure correct replacement\n",
        "        word_cleaned = re.sub(r'[^\\w\\s]', '', word.lower())\n",
        "        # Replace word if it exists in the replacements dictionary\n",
        "        new_word = replacements.get(word_cleaned, word)\n",
        "        new_words.append(new_word)\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Function to process the input file, perform word replacements, and write the modified content to the output file\n",
        "def process_file(input_file, output_file, replacements):\n",
        "\n",
        "    # Load the input file into a pandas DataFrame\n",
        "    edmunds_car_data = pd.read_csv(input_file)\n",
        "\n",
        "    # Replace words in the 'Comment' column\n",
        "    edmunds_car_data['Comment'] = edmunds_car_data['Comment'].apply(replace_words_in_text, args=(replacements,))\n",
        "\n",
        "    # Save the modified DataFrame to the output file\n",
        "    edmunds_car_data.to_csv(output_file, index=False)\n",
        "\n",
        "# Main block to run the script\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    car_models_and_brands = pd.read_csv(replacement_file)\n",
        "    replacements = load_replacements(car_models_and_brands)\n",
        "    # Process the input file and apply the replacements\n",
        "    process_file(input_file, output_file, replacements)\n",
        "\n",
        "    print(f\"File processed and saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "xrK6AwqgKEbn",
        "outputId": "47ed2b8a-4e25-4035-b6ae-0c2f3754c9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'car_models_and_brands.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fb4e6f8ebb42>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mcar_models_and_brands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacement_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mreplacements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_replacements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcar_models_and_brands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Process the input file and apply the replacements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'car_models_and_brands.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set of brands, cleaned to handle variations like punctuation and spaces\n",
        "brands_set = {\n",
        "    'acura', 'audi', 'bmw', 'buick', 'cadillac', 'chevrolet', 'chrysler', 'dodge', 'ford',\n",
        "    'honda', 'hyundai', 'infiniti', 'kia', 'lincoln', 'lexus', 'mazda', 'mercedes', 'mercury',\n",
        "    'mitsubishi', 'nissan', 'pontiac','saturn', 'subaru', 'suzuki', 'toyota', 'volkswagen', 'volvo'\n",
        "}\n",
        "\n",
        "# Normalize the brand set to handle variations\n",
        "normalized_brands = {}\n",
        "for brand in brands_set:\n",
        "    # Normalize brands by removing spaces and punctuation, and converting to lowercase\n",
        "    normalized_key = re.sub(r'[\\s\\-\\.]', '', brand).lower()\n",
        "    normalized_brands[normalized_key] = brand\n",
        "\n",
        "# Function to clean and tokenize sentences\n",
        "def clean_and_tokenize(sentence):\n",
        "    # Remove punctuation and convert text to lowercase\n",
        "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
        "    # Tokenize by splitting on whitespace\n",
        "    return sentence.split()\n",
        "\n",
        "# Function to process the input CSV file\n",
        "def process_input_file(input_filename):\n",
        "    brand_freq = defaultdict(int)\n",
        "\n",
        "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "        next(reader)  # Skip the header row\n",
        "\n",
        "        for row in reader:\n",
        "            if row:  # Ensure the row is not empty\n",
        "                text = row[-1]  # Assuming the relevant text is in the second column (e.g., \"Comment\" column)\n",
        "                tokens = clean_and_tokenize(text)\n",
        "                seen_brands = set()  # Track seen brands in this post to avoid duplicates\n",
        "\n",
        "                for token in tokens:\n",
        "                    token = re.sub(r'[^\\w\\s]', '', token)  # Remove any leftover punctuation\n",
        "                    if token in normalized_brands:\n",
        "                        seen_brands.add(normalized_brands[token])\n",
        "\n",
        "                # Increment the brand frequency for the brands seen in this post\n",
        "                for brand in seen_brands:\n",
        "                    brand_freq[brand] += 1\n",
        "\n",
        "    return brand_freq\n",
        "\n",
        "# Input file\n",
        "input_filename = 'replacement_sample_data.csv'  # Make sure this file path is correct\n",
        "\n",
        "# Get the brand frequencies\n",
        "brand_frequencies = process_input_file(input_filename)\n",
        "\n",
        "# Sort and get the top 10 brands by frequency\n",
        "top_10_brands = sorted(brand_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# Output the top 10 brands and their frequencies\n",
        "for brand, freq in top_10_brands:\n",
        "    print(f\"{brand}: {freq}\")\n",
        "\n",
        "# Optionally, write this to a CSV file\n",
        "with open('Top_10_Brands.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
        "    writer = csv.writer(outfile)\n",
        "    writer.writerow(['Brand', 'Frequency'])\n",
        "    writer.writerows(top_10_brands)\n",
        "\n"
      ],
      "metadata": {
        "id": "y56uQpy-KJW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task C\n",
        "Calculate lift ratios for associations between the top-10 brands identified in Task A. You will have\n",
        "to write a script to do this task). For lift calculations, be sure not to count a mention more than once\n",
        "per post, even if it is mentioned multiple times in the post. In your code, ensure that a message is not\n",
        "counted in the lift calculations if the mentions of two brands are separated by more than, say, 5 or 7\n",
        "words."
      ],
      "metadata": {
        "id": "rkUOp3PsEApj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize global variables and data structures\n",
        "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
        "word_frequency = defaultdict(int)  # Dictionary to store word frequency in posts\n",
        "word_pair_frequency = defaultdict(int)  # Dictionary to store word pair co-occurrence frequency\n",
        "brands_set = {\n",
        "    'acura', 'audi', 'bmw', 'buick', 'cadillac', 'chevrolet', 'chrysler', 'dodge', 'ford',\n",
        "    'honda', 'hyundai', 'infiniti', 'kia', 'lincoln', 'lexus', 'mazda', 'mercedes', 'mercury',\n",
        "    'mitsubishi', 'nissan', 'pontiac','saturn', 'subaru', 'suzuki', 'toyota', 'volkswagen', 'volvo'\n",
        "}\n",
        "\n",
        "# Normalize the brand set to handle variations\n",
        "normalized_brands = {}\n",
        "for brand in brands_set:\n",
        "    normalized_key = re.sub(r'[\\s\\-\\.]', '', brand).lower()\n",
        "    normalized_brands[normalized_key] = brand\n",
        "\n",
        "# Function to clean and tokenize sentences\n",
        "def clean_and_tokenize(sentence):\n",
        "    if isinstance(sentence, str):\n",
        "        # Remove punctuation and convert text to lowercase\n",
        "        sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
        "        # Tokenize by splitting on whitespace\n",
        "        return sentence.split()\n",
        "    return []  # Return an empty list if sentence is not a string\n",
        "\n",
        "# Process the input file and calculate word and pair frequencies\n",
        "def process_and_calculate_lift(df, max_word_distance=5):\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['Comment']\n",
        "        tokens = clean_and_tokenize(text)\n",
        "\n",
        "        # Find normalized brands in the token list and track their positions\n",
        "        brand_positions = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            token_cleaned = re.sub(r'[^\\w\\s]', '', token)  # Remove leftover punctuation\n",
        "            if token_cleaned in normalized_brands:\n",
        "                brand_positions.append((normalized_brands[token_cleaned], i))\n",
        "\n",
        "        # Count individual brand mentions\n",
        "        seen_brands = set([brand for brand, _ in brand_positions])\n",
        "        for brand in seen_brands:\n",
        "            word_frequency[brand] += 1\n",
        "\n",
        "        # Count co-occurrences of brands within the max_word_distance\n",
        "        for (brand1, pos1), (brand2, pos2) in combinations(brand_positions, 2):\n",
        "            if abs(pos1 - pos2) <= max_word_distance:\n",
        "                if brand1 != brand2:\n",
        "                    pair_key = tuple(sorted([brand1, brand2]))\n",
        "                    word_pair_frequency[pair_key] += 1\n",
        "\n",
        "# Calculate lift values\n",
        "def calculate_lift():\n",
        "    lift_values = []\n",
        "    for (brand1, brand2), pair_count in word_pair_frequency.items():\n",
        "        freq1 = word_frequency[brand1]\n",
        "        freq2 = word_frequency[brand2]\n",
        "        lift = (pair_count * file_length) / (freq1 * freq2)\n",
        "        lift_values.append((brand1, brand2, lift))\n",
        "\n",
        "    # Convert lift values into a DataFrame\n",
        "    df_lift = pd.DataFrame(lift_values, columns=['Brand1', 'Brand2', 'Lift'])\n",
        "    return df_lift\n",
        "\n",
        "# Get top 10 brands by frequency\n",
        "def get_top_10_brands():\n",
        "    sorted_brands = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_10_brands = [brand for brand, _ in sorted_brands[:10]]\n",
        "    return top_10_brands\n",
        "\n",
        "# Create a lift matrix for the top 10 brands\n",
        "def create_lift_matrix_for_top_10(df_lift, top_10_brands):\n",
        "    # Initialize an empty matrix for the top 10 brands\n",
        "    lift_matrix = pd.DataFrame(0, index=top_10_brands, columns=top_10_brands)\n",
        "\n",
        "    # Populate the matrix with lift values only for the top 10 brands\n",
        "    for _, row in df_lift.iterrows():\n",
        "        if row['Brand1'] in top_10_brands and row['Brand2'] in top_10_brands:\n",
        "            lift_matrix.loc[row['Brand1'], row['Brand2']] = row['Lift']\n",
        "            lift_matrix.loc[row['Brand2'], row['Brand1']] = row['Lift']  # Symmetric values\n",
        "\n",
        "    return lift_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'replacement_sample_data.csv'  # Ensure the correct path\n",
        "df = pd.read_csv(file_path)\n",
        "file_length = len(df)  # Number of rows in the input file\n",
        "\n",
        "# Process the file and calculate lift\n",
        "process_and_calculate_lift(df)\n",
        "\n",
        "# Get the lift values\n",
        "df_lift = calculate_lift()\n",
        "\n",
        "# Get the top 10 brands\n",
        "top_10_brands = get_top_10_brands()\n",
        "\n",
        "# Create and save the lift matrix for the top 10 brands\n",
        "lift_matrix_top_10 = create_lift_matrix_for_top_10(df_lift, top_10_brands)\n",
        "lift_matrix_top_10.to_csv('Lift_Matrix_Top_10.csv')\n",
        "\n",
        "# Or, you can display the result in the console\n",
        "# print(lift_matrix_top_10)\n",
        "\n",
        "# Display the heatmap\n",
        "sns.heatmap(lift_matrix_top_10, annot=True)\n",
        "plt.title(\"Lift Values Between Brands\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cA8dlS8aKdad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task D:\n",
        "Show the brands on a multi-dimensional scaling (MDS) map (use a Python script for MDS, there are multiple scripts available on GitHub)."
      ],
      "metadata": {
        "id": "a74kg9QHEIXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "# Load the lift matrix (for top 10 brands) from CSV\n",
        "lift_matrix_top_10 = pd.read_csv('Lift_Matrix_Top_10.csv', index_col=0)\n",
        "\n",
        "# Invert lift values for distance calculation (higher lift means closer brands)\n",
        "# Use a small epsilon to avoid division by zero\n",
        "epsilon = 1e-6\n",
        "distance_matrix = 1 / (lift_matrix_top_10 + epsilon)\n",
        "\n",
        "# Apply MDS to the distance matrix\n",
        "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
        "mds_coordinates = mds.fit_transform(distance_matrix)\n",
        "\n",
        "# Create a scatter plot of the brands in the 2D MDS space\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(mds_coordinates[:, 0], mds_coordinates[:, 1], s=100)\n",
        "\n",
        "# Annotate each brand in the plot\n",
        "for i, brand in enumerate(lift_matrix_top_10.index):\n",
        "    plt.text(mds_coordinates[i, 0], mds_coordinates[i, 1], brand, fontsize=12)\n",
        "\n",
        "plt.title('MDS Map of Top 10 Brands Based on Lift Values')\n",
        "plt.xlabel('MDS Dimension 1')\n",
        "plt.ylabel('MDS Dimension 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PyH69yeHDsqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task E:\n",
        "What insights can you offer to your client from your analyses in Tasks C and D?"
      ],
      "metadata": {
        "id": "uluJ13o2EKfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We identified a clear grouping in our MDS map. Three brands that were grouped closely together were Toyota, Honda and Nissan. In particular, Toyota and Honda had a high lift value (4.09), indicating frequent comentions between the two brands. All three brands had some of the highest lift ratios compared to any other pairs of brands, signifying that their market niche is clearly defined in the minds of customers. Even if these brands are not as traditionally associated with luxury as some other brands in our analysis, they were still talked about very frequently. Car makers should pay special attention to what steps those car companies are taking to market and design their vehicles, especially when it comes to price-sensitive customers.\n",
        "\n",
        "On the other hand, one of the most frequently mentioned brands was BMW. However, it does not seem to be frequently mentioned in conjunction with other brands — the highest lift values associated with BMW were with Audi (0.88) and Lexus (0.83), but these are relatively low, which is not significant. A lack of association in this context can indicate that BMW may be more unique compared to the other brands and possibly, it stands out as a well known entry level luxury brand. Its separation from the broader luxury clusters reflects how consumers perceive BMW as a more distinctive brand.\n",
        "\n",
        "More established luxury brands such as Lexus, Cadillac, Audi, Mercedes, Acura and Infiniti are mentioned together frequently, indicating these may be in a  competitive set; they occupy a similar niche in the market. For example, Acura and Infiniti have a very strong lift value of 2.24. Cadillac also stands out with high lift with Audi (2.22) and with Mercedes (2.13). The tendency to group them together means that more of a discussion is held among users of this forum regarding their relationships. If any one of these brands wants to get a leg up in the field, they could start by targeting the other brands in this cluster as direct competitors and clearly differentiating themselves from others in the market.\n",
        "\n"
      ],
      "metadata": {
        "id": "6vU9_78ALgtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task F"
      ],
      "metadata": {
        "id": "hPqCTmMNEa4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are 5 most frequently mentioned attributes or features of cars in the discussions? Which attributes are most strongly associated with which of these 5 brands? There is no need to plot the MDS plot for this question."
      ],
      "metadata": {
        "id": "KTvly3VnEfiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "\n",
        "# Attribute list\n",
        "attribute_list = ['performance', 'luxury', 'reliable', 'safety', 'affordable', 'favorite',\n",
        "                   'speed', 'comfort', 'warranty', 'price', 'msrp', 'security', 'fuel',\n",
        "                  'consumption', 'insurance', 'maintainence', 'dependable', 'reliablie',\n",
        "                  'engine', 'fuel', 'fast', 'gas', 'horsepower', 'mileage', 'seat', 'seats',\n",
        "                  'safety', 'technology', 'radio', 'gps','wifi', 'driverless', 'charger', 'roof',\n",
        "                  'sunroof', 'wheels', 'wheel', 'tire', 'tires', 'comfort', 'comfortable','airconditioner',\n",
        "                  'heat', 'cooling', 'cool', 'heating', 'alarm', 'wireless', 'affordable', 'cheap', 'value', 'expensive']\n",
        "\n",
        "# Clean attribute list for case-insensitive search\n",
        "attribute_list = [attr.lower() for attr in attribute_list]\n",
        "\n",
        "# Top 10 brands (assuming we have this from previous steps)\n",
        "top_10_brands = lift_matrix_top_10.columns.values\n",
        "\n",
        "# Dictionary to hold brand-attribute frequencies\n",
        "brand_attribute_freq = defaultdict(lambda: defaultdict(int))\n",
        "brand_total_mentions = defaultdict(int)  # To store the total number of mentions for each brand\n",
        "\n",
        "# Function to clean and tokenize sentences\n",
        "def clean_and_tokenize(sentence):\n",
        "    if isinstance(sentence, str):\n",
        "        # Remove punctuation and convert text to lowercase\n",
        "        sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
        "        # Tokenize by splitting on whitespace\n",
        "        return sentence.split()\n",
        "    return []\n",
        "\n",
        "# Process the input file and count brand-attribute occurrences\n",
        "def process_and_count_attributes(df):\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['Comment']\n",
        "        tokens = clean_and_tokenize(text)\n",
        "\n",
        "        # Track brands and attributes in each comment\n",
        "        brands_in_comment = set()\n",
        "        attributes_in_comment = set()\n",
        "\n",
        "        for token in tokens:\n",
        "            # Check if token is a brand\n",
        "            if token in top_10_brands:\n",
        "                brands_in_comment.add(token)\n",
        "            # Check if token is an attribute\n",
        "            if token in attribute_list:\n",
        "                attributes_in_comment.add(token)\n",
        "\n",
        "        # For each brand-attribute pair, update the frequency\n",
        "        for brand in brands_in_comment:\n",
        "            brand_total_mentions[brand] += 1  # Increment total mentions for the brand\n",
        "            for attribute in attributes_in_comment:\n",
        "                brand_attribute_freq[brand][attribute] += 1\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'replacement_sample_data.csv'  # Ensure the correct path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Process the file to count brand-attribute associations\n",
        "process_and_count_attributes(df)\n",
        "\n",
        "# Sum the overall attribute frequencies\n",
        "attribute_freq = defaultdict(int)\n",
        "for brand, attributes in brand_attribute_freq.items():\n",
        "    for attribute, freq in attributes.items():\n",
        "        attribute_freq[attribute] += freq\n",
        "\n",
        "# Get the top 5 most frequently mentioned attributes\n",
        "top_5_attributes = sorted(attribute_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "top_5_attributes = [attr for attr, _ in top_5_attributes]\n",
        "\n",
        "# Create a table showing how often each of the top 5 attributes is mentioned with each brand\n",
        "brand_attribute_table = pd.DataFrame(index=top_10_brands, columns=top_5_attributes)\n",
        "\n",
        "for brand in top_10_brands:\n",
        "    for attribute in top_5_attributes:\n",
        "        brand_attribute_table.loc[brand, attribute] = brand_attribute_freq[brand][attribute]\n",
        "\n",
        "# Fill NaN with 0 for missing values and convert to integers\n",
        "brand_attribute_table = brand_attribute_table.fillna(0).astype(int)\n",
        "\n",
        "# Normalize the scores by dividing each cell by the total number of mentions for that brand\n",
        "for brand in top_10_brands:\n",
        "    total_mentions = brand_total_mentions[brand]\n",
        "    if total_mentions > 0:\n",
        "        brand_attribute_table.loc[brand] = brand_attribute_table.loc[brand] / total_mentions\n",
        "\n",
        "# Display the normalized table\n",
        "# print(brand_attribute_table)\n",
        "\n",
        "sns.heatmap(brand_attribute_table, annot=True)\n",
        "plt.xlabel(\"Attributes\")\n",
        "plt.ylabel(\"Brand Names\")\n",
        "plt.title(\"Lift Values Between Brands and Attributes\")\n",
        "plt.show()\n",
        "\n",
        "# Optionally, save the normalized result to a CSV\n",
        "brand_attribute_table.to_csv('Normalized_Brand_Attribute_Associations.csv')\n"
      ],
      "metadata": {
        "id": "ZP9_m6sNK-jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task G"
      ],
      "metadata": {
        "id": "Xt39YRA9EvxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What advice will you give to your client from Task F?"
      ],
      "metadata": {
        "id": "humRj-l6EzCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most frequently mentioned attributes appear to be performance, luxury, and price. But since these attributes are mentioned frequently, we know that a lot of people consider these attributes. However, the poor lift associations with the top 10 brands indicate that these attributes are not being mentioned with the top 10 brands very frequently. These lift values cannot be used to derive meaningful insights on the brand’s success or failure in getting people to associate the aforementioned attributes with their products.\n",
        "\n",
        "However, these lift values suggest that the companies have an opportunity to make their brand synonymous with these attributes by running marketing campaigns, strengthening the association between their brand and these attributes in the minds of customers. These brands should explore strategies and invest more resources to implement innovative marketing campaigns.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zC8yqKX0xml0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task H"
      ],
      "metadata": {
        "id": "wZY3ZxktE3SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is the most aspirational brand in your data in terms of people actually wanting to buy or own? Describe your analysis. What are the business implications for this brand?\n"
      ],
      "metadata": {
        "id": "EcXxYhk9E6jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "\n",
        "# Aspiration list\n",
        "aspiration_list = ['aim to', 'would like', 'looking for', 'really believe', 'really need', 'looking at',\n",
        "                   'hope for', 'really hope', 'wish to', 'dream car', 'aspire', 'hope to', 'want to',\n",
        "                   'looking to', 'really want', 'can afford', 'love to']\n",
        "\n",
        "# Top 10 brands (assuming we have this from previous steps)\n",
        "top_10_brands = ['acura', 'audi', 'bmw', 'buick', 'cadillac', 'chevrolet', 'chrysler', 'dodge', 'ford',\n",
        "                 'honda', 'hyundai', 'infiniti', 'kia', 'lincoln', 'lexus', 'mazda', 'mercedes', 'mercury',\n",
        "                 'mitsubishi', 'nissan', 'pontiac','saturn', 'subaru', 'suzuki', 'toyota', 'volkswagen', 'volvo']\n",
        "\n",
        "# Initialize dictionaries to hold frequencies\n",
        "brand_freq = defaultdict(int)  # To hold occurrences of each brand\n",
        "aspiration_freq = defaultdict(int)  # To hold occurrences of each aspirational term\n",
        "brand_aspiration_freq = defaultdict(lambda: defaultdict(int))  # To hold joint occurrences\n",
        "\n",
        "# Function to clean and tokenize sentences\n",
        "def clean_and_tokenize(sentence):\n",
        "    if isinstance(sentence, str):\n",
        "        # Remove punctuation and convert text to lowercase\n",
        "        sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
        "        # Tokenize by splitting on whitespace\n",
        "        return sentence.split()\n",
        "    return []\n",
        "\n",
        "# Check if any aspirational phrase is in the text\n",
        "def contains_aspiration(text, aspiration_list):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        for phrase in aspiration_list:\n",
        "            if phrase in text.lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Process the input file and count brand-aspirational term occurrences\n",
        "def process_and_count_aspirations(df):\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['Comment']\n",
        "        tokens = clean_and_tokenize(text)\n",
        "\n",
        "        # Track brands and aspirational phrases in each comment\n",
        "        brands_in_comment = set()\n",
        "        aspirations_in_comment = set()\n",
        "\n",
        "        # Identify brands in the comment\n",
        "        for token in tokens:\n",
        "            if token in top_10_brands:\n",
        "                brands_in_comment.add(token)\n",
        "\n",
        "        # Identify aspirational phrases in the comment\n",
        "        if contains_aspiration(text, aspiration_list):\n",
        "            for phrase in aspiration_list:\n",
        "                if phrase in text.lower():\n",
        "                    aspirations_in_comment.add(phrase)\n",
        "\n",
        "        # Update counts for brands and aspirational terms\n",
        "        for brand in brands_in_comment:\n",
        "            brand_freq[brand] += 1\n",
        "            for aspiration in aspirations_in_comment:\n",
        "                aspiration_freq[aspiration] += 1\n",
        "                brand_aspiration_freq[brand][aspiration] += 1\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'replacement_sample_data.csv'  # Ensure the correct path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Process the file to count brand-aspirational associations\n",
        "process_and_count_aspirations(df)\n",
        "\n",
        "# Calculate the lift values and store them in a matrix\n",
        "lift_matrix = pd.DataFrame(index=top_10_brands, columns=aspiration_list)\n",
        "\n",
        "total_comments = len(df)\n",
        "\n",
        "for brand in top_10_brands:\n",
        "    for aspiration in aspiration_list:\n",
        "        # Joint probability: P(B ∩ A)\n",
        "        joint_occurrences = brand_aspiration_freq[brand][aspiration]\n",
        "        P_B_and_A = joint_occurrences / total_comments\n",
        "\n",
        "        # Probability of brand: P(B)\n",
        "        brand_occurrences = brand_freq[brand]\n",
        "        P_B = brand_occurrences / total_comments\n",
        "\n",
        "        # Probability of aspirational term: P(A)\n",
        "        aspiration_occurrences = aspiration_freq[aspiration]\n",
        "        P_A = aspiration_occurrences / total_comments\n",
        "\n",
        "        # Calculate lift if P(B) and P(A) are not zero\n",
        "        if P_B > 0 and P_A > 0:\n",
        "            lift = P_B_and_A / (P_B * P_A)\n",
        "            lift_matrix.loc[brand, aspiration] = lift\n",
        "        else:\n",
        "            lift_matrix.loc[brand, aspiration] = 0  # If no occurrences, set lift to 0\n",
        "\n",
        "# Display the matrix\n",
        "#print(lift_matrix)\n",
        "for cols in lift_matrix.columns:\n",
        "    lift_matrix[cols] = lift_matrix[cols].astype(float)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.heatmap(lift_matrix, annot=True)\n",
        "plt.xlabel(\"Aspirational Phrases\")\n",
        "plt.ylabel(\"Brand Names\")\n",
        "plt.title(\"Lift Values Between Brands and Aspirational Phrases\")\n",
        "plt.show()\n",
        "\n",
        "# Optionally, save the lift matrix to a CSV file\n",
        "lift_matrix.to_csv('Brand_Aspiration_Lift_Matrix.csv')\n"
      ],
      "metadata": {
        "id": "7TzJdmEvLGC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_lift = pd.DataFrame(lift_matrix.median(axis=1)).rename(columns={0: 'Median Lift Value'})\n",
        "median_lift.sort_values(by='Median Lift Value', ascending=False)"
      ],
      "metadata": {
        "id": "5j1ExEOMCN-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach in determining what it meant to be aspirational centered around identifying aspirational phrases though bigrams. In order to get “aspirational phrases”, we looked at the lift values between phrases that might imply aspiration and the brands we analyzed. Because “dream” and “dream of” can have very different connotations, for example, bigrams better refined our results compared to a simple bag of words approach. Also, we need to consider the fact that the messages are being posted in an informal setting. Thus, people are more likely to use very casual phrases that are likely to appear in conversations with friends or in a low stress environment. Therefore, we include phrases such as “really want” as they may be likely to show up in the corpus. We also used ChatGPT to brainstorm some other slang phrases used in the early 2000’s such as “Big Pimpin’“, “Livin’ Large,” and “Ballin’.”\n",
        "\n",
        "Many of the lift values were not very significant as many were less than or equal to 1. However, Infiniti stood out by having the highest median value for lift associations with aspirational phrases. This can indicate that there is at least some merit to Infiniti that makes it stand out in comparison to other brands. It is likely that people hold some promise or belief in the brand, meaning Infiniti may have gained some consumer trust. This means that new customers may be more receptive to marketing attempts.\n",
        "\n",
        "Having an “aspirational” brand backed up by this analysis gives brands some fire power when it comes to marketing. Especially in head-to-head comparative marketing strategies, being able to say that your brand “is more aspirational” than another brand (or generally more aspirational than other top car brands) works great as a way to sell the idea of the car to new potential customers. When combining the findings of our research into lift values between cars and these lift values regarding aspirations, a company like Toyota could emphasize how more people see its brand as touting a “dream car” than Honda, part of their competitive set. With information like this, there are a staggering number of avenues where a car brand can tout their superiority over competing brands, citing “the people” (or, in this case, online forums) as their source. And whom do customers trust more than themselves, represented by the people?\n",
        "\n"
      ],
      "metadata": {
        "id": "S_cWIGS4xscf"
      }
    }
  ]
}