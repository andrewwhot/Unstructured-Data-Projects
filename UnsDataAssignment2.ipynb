{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Team Members:\n",
        "#Jason Antal, Ari Pai, Mykyta Zavhorodko, Albert Nguyen, Andrew White\n",
        "#11am-1pm Cohort"
      ],
      "metadata": {
        "id": "XxuSHLdnzkbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDfsBhI_Rxqy"
      },
      "outputs": [],
      "source": [
        "#Part A, extract data\n",
        "\n",
        "#requires manually installing files found at https://github.com/mozilla/geckodriver/releases\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Fetch beer links from the page\n",
        "def get_beer_links(url):\n",
        "    #CHANGE BROWSER HERE AND BELOW IF NEEDED\n",
        "    firefox_options = Options()\n",
        "    firefox_options.add_argument(\"--headless\")\n",
        "    service = Service(r\"C:\\Users\\jason\\Downloads\\geckodriver-v0.35.0-win64\\geckodriver.exe\")  # UPDATE FILEPATH HERE\n",
        "    driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "    driver.set_page_load_timeout(60000)\n",
        "    driver.get(url)\n",
        "    links = []\n",
        "    # Extract all links\n",
        "    td_elements_with_links = driver.find_elements(By.XPATH, \"//td[a]\")\n",
        "    for td in td_elements_with_links:\n",
        "        a_tag = td.find_element(By.TAG_NAME, 'a')\n",
        "        href = a_tag.get_attribute('href')\n",
        "        links.append(href)\n",
        "    driver.quit()\n",
        "    return links\n",
        "\n",
        "# Scrape reviews from each individual page using links\n",
        "def scrape_review(url):\n",
        "    # CHANGE BROWSER HERE AND ABOVE IF NEEDED\n",
        "    firefox_options = Options()\n",
        "    firefox_options.add_argument(\"--headless\")\n",
        "    service = Service(r\"C:\\Users\\jason\\Downloads\\geckodriver-v0.35.0-win64\\geckodriver.exe\")  # UPDATE FILEPATH HERE\n",
        "    driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "    driver.set_page_load_timeout(6000)  # Set timeout to 100 minutes\n",
        "    driver.get(url)\n",
        "\n",
        "    #Get beer name and brewery name\n",
        "    try:\n",
        "        title_bar = driver.find_element(By.XPATH, \"//div[@class='titleBar']\")\n",
        "        beer_name = title_bar.find_element(By.XPATH, \".//h1\").text.strip()\n",
        "        brewery_name = title_bar.find_element(By.XPATH, \".//span\").text.strip()\n",
        "    except NoSuchElementException:\n",
        "        beer_name = \"N/A\"\n",
        "        brewery_name = \"N/A\"\n",
        "\n",
        "    reviews = []\n",
        "    # Find review containers\n",
        "    review_divs = driver.find_elements(By.XPATH, \"//div[@class='user-comment']\")\n",
        "    for review in review_divs:\n",
        "        try:\n",
        "            # Get the rating\n",
        "            try:\n",
        "                rating_span = review.find_element(By.XPATH, \".//span[contains(@class, 'BAscore_norm')]\")\n",
        "                rating = float(rating_span.text.strip()) if rating_span.text else None\n",
        "            except NoSuchElementException:\n",
        "                rating = 'N/A'\n",
        "            # Get the review text\n",
        "            try:\n",
        "                review_body = review.find_element(By.XPATH, \".//div[@style='margin:20px 0px; font-size:11pt; line-height:1.4;']\")\n",
        "                text = review_body.text.strip() if review_body else ''\n",
        "            except NoSuchElementException:\n",
        "                text = ''\n",
        "            reviews.append({\n",
        "                'beer name': beer_name,\n",
        "                'brewery name': brewery_name,\n",
        "                'rating': rating,\n",
        "                'text': text\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error while scraping review: {e}\")\n",
        "    driver.quit()\n",
        "    return reviews\n",
        "# Calls both beerlinks and scraper, then outputs into csv\n",
        "def main():\n",
        "    base_url = 'https://www.beeradvocate.com/beer/top-rated/'\n",
        "    csv_filename = 'BeerAdvocate_Reviews.csv'\n",
        "    total_reviews = 0\n",
        "    max_reviews = 5000  # Adjust as needed, comment this and its corresponding code out to speed up the script if you want to extract all reviews on the page\n",
        "    # Call get_beer_links\n",
        "    beer_links = get_beer_links(base_url)\n",
        "    # CSV\n",
        "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['beer name', 'brewery name', 'rating', 'text']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        # Scrape reviews for each beer link\n",
        "        for link in beer_links:\n",
        "            if total_reviews >= max_reviews: #Comment this out if needed\n",
        "                break\n",
        "            reviews = scrape_review(link)\n",
        "            for review in reviews:\n",
        "                writer.writerow(review)\n",
        "                total_reviews += 1 #Comment this out if needed\n",
        "                print(f\"Total reviews scraped: {total_reviews}\")\n",
        "    print(f\"Scraping completed. Total reviews collected: {total_reviews}\")\n",
        "# Execute script\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part B, Word frequency analysis and lift analysis\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import ast\n",
        "import seaborn as sns\n",
        "\n",
        "# Change filepath here if necessary\n",
        "df = pd.read_csv(r\"BeerAdvocate_New.csv\")\n",
        "\n",
        "# Convert the 'text' column to list\n",
        "df['text'] = df['text'].apply(ast.literal_eval)\n",
        "\n",
        "# Word frequency analysis\n",
        "stopwords = {'note', 'taste', 'head', 'beer', 'like', 'one', 'overall', 'nose', 'carbonation', 'notes'} #unhelpful stop words removed here\n",
        "all_words = [word for review in df['text'] for word in review if word not in stopwords]\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Plot the most common words, I chose to display 20 here\n",
        "plt.figure(figsize=(10, 5))\n",
        "word_freq.most_common(20)[::-1]\n",
        "plt.barh(*zip(*word_freq.most_common(20)))\n",
        "plt.title('Top 20 Most Common Words in Beer Reviews')\n",
        "plt.xlabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Select top attributes to calculate lift values, I chose to display 12 here\n",
        "top_attributes = [word for word, _ in word_freq.most_common(12)]\n",
        "\n",
        "# Function to check if an attribute is present in a review\n",
        "def has_attribute(review, attribute):\n",
        "    return int(attribute in review)\n",
        "\n",
        "# Create columns for each attribute\n",
        "for attr in top_attributes:\n",
        "    df[attr] = df['text'].apply(lambda x: has_attribute(x, attr))\n",
        "\n",
        "# Lift analysis\n",
        "def calculate_lift(df, attr1, attr2):\n",
        "    total_reviews = len(df)\n",
        "    support_attr1 = df[attr1].sum() / total_reviews\n",
        "    support_attr2 = df[attr2].sum() / total_reviews\n",
        "    support_both = ((df[attr1] == 1) & (df[attr2] == 1)).sum() / total_reviews\n",
        "\n",
        "    if support_attr1 * support_attr2 > 0:\n",
        "        return support_both / (support_attr1 * support_attr2)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Calculate lift for all pairs of attributes\n",
        "lift_matrix = pd.DataFrame(index=top_attributes, columns=top_attributes, dtype = float)\n",
        "\n",
        "for attr1, attr2 in combinations(top_attributes, 2):\n",
        "    lift = calculate_lift(df, attr1, attr2)\n",
        "    lift_matrix.loc[attr1, attr2] = lift\n",
        "    lift_matrix.loc[attr2, attr1] = lift\n",
        "\n",
        "for attr in top_attributes:\n",
        "    lift_matrix.loc[attr, attr] = 1.0\n",
        "\n",
        "print(lift_matrix)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(lift_matrix, annot=True, cmap='YlGnBu', fmt='.2f')\n",
        "plt.title('Lift Analysis of Top 12 Beer Attributes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LPzMu4EOc5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "6680418f-7879-451d-d845-ab98ba3833d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jason\\\\Downloads\\\\Cleaned Beer Text.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-719ce1678f07>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Change filepath here if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\jason\\Downloads\\Cleaned Beer Text.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert the 'text' column to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jason\\\\Downloads\\\\Cleaned Beer Text.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part C"
      ],
      "metadata": {
        "id": "r_q6HJTPhBuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the data\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
        "    and tokenizing it, ignoring any stopwords.\n",
        "    \"\"\"\n",
        "    # Remove punctuation and convert text to lowercase\n",
        "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', text.lower())\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    return ' '.join([word for word in sentence.split() if word not in stopwords])"
      ],
      "metadata": {
        "id": "QMYbn5UhivHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "cleaned_beer = pd.read_csv('BeerAdvocate_Reviews2.csv')\n",
        "cleaned_beer = cleaned_beer.dropna()\n",
        "cleaned_beer['text'] = cleaned_beer['text'].apply(str)\n",
        "cleaned_beer['text'] = cleaned_beer['text'].apply(clean_text)\n",
        "\n",
        "# Input the product attributes\n",
        "attributes = []\n",
        "file_name = ''\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        attributes = [l.rstrip() for l in f]\n",
        "except:\n",
        "    three_atts = str(input(\"Enter three attributes comma separated.\"))\n",
        "    attributes = re.findall('\\w+', three_atts)\n",
        "    print(attributes)\n",
        "\n",
        "# Reviews\n",
        "reviews = cleaned_beer['text'].tolist()\n",
        "\n",
        "# Add the document as a \"review\"\n",
        "reviews.append(' '.join(attributes))\n",
        "\n",
        "similarity_scores = []\n",
        "\n",
        "# Create a countvectorizer element on the reviews\n",
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# Normalize the vectors\n",
        "scaler = MaxAbsScaler()\n",
        "X_trans = scaler.fit_transform(X)\n",
        "\n",
        "# Take the last row as the reference point\n",
        "attrs_vector = X_trans[-1]\n",
        "\n",
        "for review_vector in X_trans:\n",
        "    # Calculate the similarity score\n",
        "    similarity_scores.append(cosine_similarity(review_vector, attrs_vector)[0][0])\n",
        "\n",
        "cleaned_beer['similiarity'] = similarity_scores[:-1]"
      ],
      "metadata": {
        "id": "3ZCZPxlOOd1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part D"
      ],
      "metadata": {
        "id": "ZIULhqRu3VcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "'''# Load the CSV file\n",
        "file_path = 'BeerAdvocate_Reviews2.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(file_path)'''\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score using VADER\n",
        "def get_vader_sentiment(review):\n",
        "    if pd.isna(review):\n",
        "        return None  # Return None for NaN reviews\n",
        "    return analyzer.polarity_scores(review)['compound']\n",
        "\n",
        "# Apply the sentiment analysis and create a new column for the sentiment scores, including NaN for NULL reviews\n",
        "cleaned_beer['sentiment_score'] = clean_beer['text'].apply(get_vader_sentiment)\n",
        "\n",
        "# Save the updated DataFrame with the sentiment scores\n",
        "'''output_path = 'BeerAdvocate_Reviews_with_VADER_Sentiment.csv'  # Specify the desired output path\n",
        "df.to_csv(output_path, index=False)'''"
      ],
      "metadata": {
        "id": "sx1azCxKeI3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1db8d650-8b0a-4966-acb6-35785e7da15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vaderSentiment'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-08a6fa4566c5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m '''# Load the CSV file\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BeerAdvocate_Reviews2.csv'\u001b[0m  \u001b[0;31m# Replace with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part E"
      ],
      "metadata": {
        "id": "Ddikbwj2kx3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the evaluation score for each review\n",
        "# Multiply cosine similarity and sentiment score to get evaluation score\n",
        "cleaned_beer['evaluation_score'] = cleaned_beer['similiarity'] * cleaned_beer['sentiment_score']\n",
        "\n",
        "# Group by brand and calculate the average evaluation score\n",
        "beer_scores = cleaned_beer.groupby('beer name')['evaluation_score'].mean().reset_index()\n",
        "\n",
        "# Sort and select the top 3 brands\n",
        "top_3_beers = beer_scores.sort_values(by='evaluation_score', ascending=False).head(3)\n",
        "print(top_3_beers)"
      ],
      "metadata": {
        "id": "OXFenmNck1Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part F"
      ],
      "metadata": {
        "id": "JvcWBSh8iBdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv('/BeerAdvocate_Reviews.csv').dropna()\n",
        "df['beer name'] = df['beer name'].str.split('\\n').str[0]\n",
        "\n",
        "#filtering the comments from special symbols, stop words, and punctuation\n",
        "non_attribute = ['beer','even','ipa','good','great','near']\n",
        "\n",
        "df['text_filtered'] = df['text'].apply(lambda x: x.lower())\n",
        "\n",
        "#removing all punctuation and special symbols\n",
        "df['text_filtered'] = df['text_filtered'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# Replace newline characters with spaces\n",
        "df['text_filtered'] = df['text_filtered'].str.replace('\\n', ' ', regex=False)\n",
        "\n",
        "# Remove stopwords and non-characteristic words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text_filtered'] = df['text_filtered'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words and word not in non_attribute]))\n",
        "\n",
        "words = [\"bold, strong, dark\"] #################################################THESE ARE YOUR WORDS\n",
        "\n",
        "words_doc = nlp(\" \".join(words))\n",
        "\n",
        "def compute_similarity(text):\n",
        "    doc = nlp(text)\n",
        "    return words_doc.similarity(doc)\n",
        "\n",
        "df['spacy similarity'] = df['text_filtered'].apply(compute_similarity)\n",
        "df.to_csv('BeerAdvocate_Reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "hDlXI-47iArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part G"
      ],
      "metadata": {
        "id": "Q3bMCUw0lpbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the evaluation score for each review\n",
        "# Multiply cosine similarity and sentiment score to get evaluation score\n",
        "cleaned_beer['evaluation_score'] = cleaned_beer['similiarity'] * cleaned_beer['sentiment_score']\n",
        "\n",
        "# Group by brand and calculate the average evaluation score\n",
        "beer_scores = cleaned_beer.groupby('beer name')['evaluation_score'].mean().reset_index()\n",
        "\n",
        "# Sort and select the top 3 brands\n",
        "top_3_beers = beer_scores.sort_values(by='evaluation_score', ascending=False).head(3)\n",
        "print(top_3_beers)"
      ],
      "metadata": {
        "id": "ADSiItCHv3bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_3_beer_names = top_3_beers['beer name']\n",
        "\n",
        "top_3_beer_data = cleaned_beer[cleaned_beer['beer name'].isin(top_3_beer_names)]\n",
        "\n",
        "average_ratings_top_3 = top_3_beer_data.groupby('beer name')['rating'].mean().reset_index()\n",
        "\n",
        "print(average_ratings_top_3) # Gives the average rating for each of the highest similarity/sentiment beers"
      ],
      "metadata": {
        "id": "4Y9kLf4OoLbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beer_ratings = cleaned_beer.groupby('beer name')['rating'].mean().reset_index()\n",
        "top_3_ratings = beer_ratings.sort_values(by='rating', ascending=False).head(3)\n",
        "print(top_3_ratings) # Gives the average rating for each of the highest Rated beers"
      ],
      "metadata": {
        "id": "vFcNt2WXlq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge\n",
        "joined_clean_beer = pd.merge(cleaned_beer, df['spacy similarity'], left_index=True, right_index=True)\n",
        "joined_clean_beer['spacy_evaluation_score'] = joined_clean_beer['spacy similarity'] * joined_clean_beer['sentiment_score']\n",
        "\n",
        "joined_clean_beer.groupby('beer name')[['spacy_evaluation_score', 'evaluation_score', 'rating']].mean().sort_values(by='spacy_evaluation_score', ascending=False)[:3]"
      ],
      "metadata": {
        "id": "nEr_AzZLvw0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the highest rated beers, what were their spacy similarity scores?\n",
        "top3_ratings_beers = top_3_ratings['beer name'].tolist()\n",
        "cleaned_beer_spacy.loc[top3_ratings_beers]"
      ],
      "metadata": {
        "id": "vKWt8peW2ZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part H"
      ],
      "metadata": {
        "id": "ar5JYJKw2cEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv('BeerAdvocate_New.csv').dropna()\n",
        "df['beer name'] = df['beer name'].str.split('\\n').str[0]\n",
        "\n",
        "#filtering the comments from special symbols, stop words, and punctuation\n",
        "non_attribute = ['beer','even','ipa','good','great','near', 'one', 'overall', 'i', 've', '2022', 'taste', 'bottle', 'bit', 'abv']\n",
        "\n",
        "df['text_filtered'] = df['text'].apply(lambda x: x.lower())\n",
        "\n",
        "#removing all punctuation and special symbols\n",
        "df['text_filtered'] = df['text_filtered'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# Replace newline characters with spaces\n",
        "df['text_filtered'] = df['text_filtered'].str.replace('\\n', ' ', regex=False)\n",
        "\n",
        "# Remove stopwords and non-characteristic words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text_filtered'] = df['text_filtered'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words and word not in non_attribute]))\n",
        "\n",
        "#word counts for every beer\n",
        "# Step 1: Group by 'beer name' and aggregate the 'text_filtered' column\n",
        "grouped = df.groupby('beer name')['text_filtered'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Step 2: Define a function to count the word frequencies for each beer\n",
        "def count_words(text):\n",
        "    words = text.split()  # Split the text into individual words\n",
        "    word_counts = Counter(words)  # Count the occurrences of each word\n",
        "    sorted_word_counts = dict(word_counts.most_common())  # Sort by frequency\n",
        "    return sorted_word_counts\n",
        "\n",
        "# Step 3: Apply the word count function to the 'text_filtered' column\n",
        "grouped['word_counts'] = grouped['text_filtered'].apply(count_words)\n",
        "\n",
        "# Step 4: Create a new dataframe that contains each beer and the sorted word counts\n",
        "beer_word_counts_df = grouped[['beer name', 'word_counts']]\n",
        "\n",
        "# Step 1: Group by 'beer name' and collect all 'text_filtered' comments for each beer\n",
        "grouped = df.groupby('beer name')['text_filtered'].apply(list).reset_index()\n",
        "\n",
        "# Step 2: Define a function to calculate the percentage of comments containing each word\n",
        "def word_percentage_in_comments(comments):\n",
        "    total_comments = len(comments)  # Total number of comments for the beer\n",
        "    word_in_comments = Counter()  # To count how many comments contain each word\n",
        "\n",
        "    for comment in comments:\n",
        "        words_in_comment = set(comment.split())  # Unique words in each comment\n",
        "        word_in_comments.update(words_in_comment)  # Increment count for each word\n",
        "\n",
        "    # Calculate the percentage of comments that include each word\n",
        "    word_percentages = {word: (count / total_comments) * 100 for word, count in word_in_comments.items()}\n",
        "\n",
        "    # Sort by percentage in descending order\n",
        "    sorted_word_percentages = dict(sorted(word_percentages.items(), key=lambda item: item[1], reverse=True)[:5]) #you choose how many words to include in a vector\n",
        "\n",
        "    return sorted_word_percentages\n",
        "\n",
        "# Step 3: Apply the percentage calculation function to the 'text_filtered' column\n",
        "grouped['word_percentages'] = grouped['text_filtered'].apply(word_percentage_in_comments)\n",
        "\n",
        "# Step 4: Add a column with the number of comments for each beer\n",
        "grouped['comment_count'] = grouped['text_filtered'].apply(len)\n",
        "\n",
        "\n",
        "# Step 5: Create a function to get the composite vector for a beer based on its word_percentages\n",
        "def get_beer_doc(word_percentages):\n",
        "    words = ' '.join(word_percentages.keys())  # Create a single string with the beer's most common words\n",
        "    doc = nlp(words)  # Convert the string into a spaCy Doc object\n",
        "    return doc\n",
        "\n",
        "# Step 6: Apply the function to get a spaCy Doc for each beer\n",
        "grouped['beer_doc'] = grouped['word_percentages'].apply(get_beer_doc)\n",
        "\n",
        "# Step 7: Create a function to get the vector for the user's input\n",
        "def get_user_input_doc(attributes):\n",
        "    return nlp(' '.join(attributes))  # Convert the user's input words into a spaCy Doc object\n",
        "\n",
        "######################################################################################\n",
        "user_attributes = ['orange', 'head', 'carbonation', 'light', 'bitterness'] #attributes\n",
        "######################################################################################\n",
        "\n",
        "# Step 8: Example user input (three attributes)\n",
        "user_doc = get_user_input_doc(user_attributes)\n",
        "\n",
        "# Step 9: Calculate similarity using spaCy's built-in similarity method\n",
        "grouped['similarity_score'] = grouped['beer_doc'].apply(lambda x: x.similarity(user_doc))\n",
        "\n",
        "# Step 10: Sort beers by similarity score to recommend the most relevant ones\n",
        "recommended_beers = grouped.sort_values(by='similarity_score', ascending=False)\n",
        "\n",
        "names = ['Emperor Julius','Swish', 'I Let My Tape Rock', 'Trappistes Rochefort 10', 'Coffee Cinnamon B-Bomb', 'Pseudo Sue - Double Dry-Hopped', 'Mocha Wednesday',\n",
        "'All That Is And All That Ever Will Be',\n",
        "'Notorious Triple IPA',\n",
        "'Vanilla Bean Assassin',\n",
        "'Parabola'\n",
        "]\n",
        "recommended_beers[recommended_beers['beer name'].isin(names)][[\"beer name\", \"comment_count\", \"beer_doc\", \"similarity_score\"]]\n",
        "\n"
      ],
      "metadata": {
        "id": "vFIjvFl72bhd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}